{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0zs3TflcAEr"
   },
   "source": [
    "1. Download all the data in [this folder](https://drive.google.com/open?id=1Z4TyI7FcFVEx8qdl4jO9qxvxaqLSqoEu). or [kaggle link](https://www.kaggle.com/brahma0545/aaic-assignment-tl) it contains two file both images and labels. The label file list the images and their categories in the following format:\n",
    "<b>path/to/the/image.tif,category</b>\n",
    "            \n",
    "    where the categories are numbered 0 to 15, in the following order:\n",
    "\n",
    "<b>\n",
    "\n",
    "- 0 letter    \n",
    "- 1 form\n",
    "- 2 email\n",
    "- 3 handwritten\n",
    "- 4 advertisement\n",
    "- 5 scientific report\n",
    "- 6 scientific publication\n",
    "- 7 specification\n",
    "- 8 file folder\n",
    "- 9 news article\n",
    "- 10 budget\n",
    "- 11 invoice\n",
    "- 12 presentation\n",
    "- 13 questionnaire\n",
    "- 14 resume\n",
    "- 15 memo</b>\n",
    "    \n",
    "2. On this image data, you have to train 3 types of models as given below. You have to split the data into Train and Validation data.\n",
    "\n",
    "3. Try not to load all the images into memory, use the gernarators that we have given the reference notebooks to load the batch of images only during the train data.\n",
    "or you can use this method also\n",
    "<a href='https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1'>keras-imagedatagenerator-with-flow-from-dataframe</a>\n",
    "\n",
    "<a href='https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c'>keras-flow-from-dataframe</a>\n",
    "\n",
    "\n",
    "4. You are free to choose Learning rate, optimizer, loss function, image augmentation, any hyperparameters. but you have to use the same architechture what we are asking below. \n",
    "\n",
    "5. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "Note: **fit_genarator()** method will have problems with the tensorboard histograms, try to debug it, if you could not do use histgrams=0 i.e don't include histograms, check the documentation of tensorboard for more information. \n",
    "\n",
    "6. You can check about Transfer Learning in this link - <a href='https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html'>building-powerful-image-classification-models-using-very-little-data</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, Activation, Dropout, Flatten\n",
    "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imagesv/v/o/h/voh71d00/509132755+-2755.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imagesl/l/x/t/lxt19d00/502213303.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imagesx/x/e/d/xed05a00/2075325674.tif</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imageso/o/j/b/ojb60d00/517511301+-1301.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imagesq/q/z/k/qzk17e00/2031320195.tif</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path label\n",
       "0  imagesv/v/o/h/voh71d00/509132755+-2755.tif     3\n",
       "1        imagesl/l/x/t/lxt19d00/502213303.tif     3\n",
       "2       imagesx/x/e/d/xed05a00/2075325674.tif     2\n",
       "3  imageso/o/j/b/ojb60d00/517511301+-1301.tif     3\n",
       "4       imagesq/q/z/k/qzk17e00/2031320195.tif     7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"labels_final.csv\")\n",
    "df['label'] = df['label'].map(str)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "labels = ['letter','form','email','handwritten','advertisement','scientific report','scientific publication','specification',\n",
    "          'file folder','news article','budget','invoice','presentation','questionnaire','resume','memo',]\n",
    "\n",
    "dic1 = Counter(df['label'].map(int).map(dict(enumerate(labels))))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(dic1.keys(), dic1.values())\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df['path'], df['label'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:(36000, 2),   val size:(12000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.concat((X_train, y_train), axis=1)\n",
    "df_val = pd.concat((X_val, y_val), axis=1)\n",
    "# df_test = pd.concat((X_test, y_test), axis=1)\n",
    "\n",
    "# print(\"train size:{},   val size:{},   test size:{}\".format(df_train.shape, df_val.shape, df_test.shape))\n",
    "print(\"train size:{},   val size:{}\".format(df_train.shape, df_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36000 validated image filenames belonging to 16 classes.\n",
      "Found 12000 validated image filenames belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [256, 128]  # height*width\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "#                                    horizontal_flip=True,\n",
    "                                   fill_mode='nearest',\n",
    "                                  )\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                                              dataframe = df_train,\n",
    "                                              directory = \"data_final\",\n",
    "                                              x_col = \"path\",\n",
    "                                              y_col = \"label\",\n",
    "                                              class_mode = \"categorical\",\n",
    "                                              target_size = IMAGE_SIZE,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              )\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "                                              dataframe = df_val,\n",
    "                                              directory = \"data_final\",\n",
    "                                              x_col = \"path\",\n",
    "                                              y_col = \"label\",\n",
    "                                              class_mode = \"categorical\",\n",
    "                                              target_size = IMAGE_SIZE,\n",
    "                                              batch_size = BATCH_SIZE\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 128, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.imshow(train_generator[0][0][3]);\n",
    "train_generator[0][0][3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZXpEZtJcAEu"
   },
   "source": [
    "# Model-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EF12MYu1cAEy"
   },
   "source": [
    "- 1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights. \n",
    "- 2. After VGG-16 network without FC layers, add a new Conv block ( 1 Conv layer and 1 Maxpooling ), 2 FC layers and a output layer to classify 16 classes. You are free to choose any hyperparameters/parameters of conv block, FC layers, output layer. \n",
    "- 3. Final architecture will be <b>INPUT --> VGG-16 without Top layers(FC) --> Conv Layer --> Maxpool Layer --> 2 FC layers --> Output Layer</b>\n",
    "- 4. Train only new Conv block, FC layers, output layer. Don't train the VGG-16 network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (InputLayer)     [(None, 256, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 8, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 8, 4, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "Pool1 (MaxPooling2D)         (None, 4, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 18,126,032\n",
      "Trainable params: 3,411,344\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# https://keras.io/getting-started/faq/ #how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "# Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
    "# Varibles will also set to some value from before session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', input_shape=IMAGE_SIZE+[3], include_top=False)\n",
    "vgg16.trainable = False\n",
    "\n",
    "input_layer = Input(shape=IMAGE_SIZE+[3], name='Input_Layer')\n",
    "\n",
    "# x = vgg16(input_layer, training=False)\n",
    "vgg = vgg16(input_layer)\n",
    "\n",
    "Conv1 = Conv2D(256, (3,3), padding='same', data_format='channels_last', activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=0), name='Conv1')(vgg)\n",
    "Pool1 = MaxPool2D(name='Pool1')(Conv1)\n",
    "\n",
    "flatten = Flatten(data_format='channels_last', name='Flatten')(Pool1)\n",
    "D1 = Dropout(0.5)(flatten)\n",
    "\n",
    "FC1 = Dense(units=1024, activation='relu', kernel_initializer=tf.keras.initializers.glorot_normal(seed=32),name='FC1')(D1)\n",
    "D2 = Dropout(0.5)(FC1)\n",
    "\n",
    "FC2 = Dense(units=128, activation='relu', kernel_initializer=tf.keras.initializers.glorot_normal(seed=33),name='FC2')(D2)\n",
    "D3 = Dropout(0.5)(FC2)\n",
    "\n",
    "Out = Dense(units=16, activation='softmax', kernel_initializer=tf.keras.initializers.glorot_normal(seed=3),name='Output')(D3)\n",
    "\n",
    "#Creating a model\n",
    "model1 = Model(inputs=input_layer, outputs=Out)\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model1, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# !rmdir /s /q logs\\model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2250/2250 [==============================] - 7273s 3s/step - loss: 15.1111 - accuracy: 0.0621 - val_loss: 15.0906 - val_accuracy: 0.0637\n",
      "Epoch 2/10\n",
      "2250/2250 [==============================] - 7583s 3s/step - loss: 15.1054 - accuracy: 0.0628 - val_loss: 15.1188 - val_accuracy: 0.0620\n",
      "Epoch 3/10\n",
      "2079/2250 [==========================>...] - ETA: 8:51 - loss: 15.1141 - accuracy: 0.0623"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-87fef0d88f58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#                     callbacks=callbacks_list,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    576\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m   \u001b[0mshape_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[1;31m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m    503\u001b[0m   \"\"\"\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9032\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   9033\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ShapeN\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9034\u001b[1;33m         name, _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[0;32m   9035\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9036\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_dir=\"logs\\\\model1\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "\n",
    "steps_per_epoch = ceil(X_train.shape[0]/BATCH_SIZE)\n",
    "model1.fit_generator(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=10,\n",
    "#                     callbacks=callbacks_list,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "De0UlsaOcAE1"
   },
   "source": [
    "# Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNXN3EXFcAE5"
   },
   "source": [
    "<pre>\n",
    "1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights.\n",
    "2. After VGG-16 network without FC layers, don't use FC layers, use conv layers only as Fully connected layer. any FC layer can be converted to a CONV layer. This conversion will reduce the No of Trainable parameters in FC layers. For example, an FC layer with K=4096 that is looking at some input volume of size 77512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be 114096 since only a single depth column fits across the input volume, giving identical result as the initial FC layer. You can refer <a href='http://cs231n.github.io/convolutional-networks/#convert'>this</a> link to better understanding of using Conv layer in place of fully connected layers.\n",
    "3. Final architecture will be VGG-16 without FC layers(without top), 2 Conv layers identical to FC layers, 1 output layer for 16 class classification. <b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>\n",
    "3. Train only last 2 Conv layers identical to FC layers, 1 output layer. Don't train the VGG-16 network. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', input_shape=IMAGE_SIZE+[3], include_top=False)\n",
    "vgg16.trainable = False\n",
    "\n",
    "#Input layer\n",
    "input_layer = Input(shape=IMAGE_SIZE+[3], name='Input_Layer')\n",
    "\n",
    "x = vgg16(input_layer, training=False)\n",
    "\n",
    "Conv1 = Conv2D(256, (4,4), data_format='channels_last', activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=0), name='Conv1')(x)\n",
    "\n",
    "Conv2 = Conv2D(64, (1,1), data_format='channels_last', activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=0), name='Conv2')(Conv1)\n",
    "\n",
    "flatten = Flatten(data_format='channels_last',name='Flatten')(Conv2)\n",
    "\n",
    "#output layer\n",
    "Out = Dense(units=16,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=3),name='Output')(flatten)\n",
    "\n",
    "#Creating a model\n",
    "model2 = Model(inputs=input_layer, outputs=Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model2, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# !rmdir /s /q logs\\model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "log_dir=\"logs\\\\model2\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "model2.fit_generator(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=216,\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amKbfojfcAE-"
   },
   "source": [
    "# Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9AULF-PcAFC"
   },
   "source": [
    "<pre>\n",
    "1. Use same network as Model-2 '<b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>' and train only Last 6 Layers of VGG-16 network, 2 Conv layers identical to FC layers, 1 output layer.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', input_shape=IMAGE_SIZE+[3], include_top=False)\n",
    "\n",
    "for layer in vgg16.layers[0:11]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Input layer\n",
    "input_layer = Input(shape=IMAGE_SIZE+[3], name='Input_Layer')\n",
    "\n",
    "x = vgg16(input_layer)\n",
    "\n",
    "Conv1 = Conv2D(256, (4,4), data_format='channels_last', activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=0), name='Conv1')(x)\n",
    "\n",
    "Conv2 = Conv2D(64, (1,1), data_format='channels_last', activation='relu',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=0), name='Conv2')(Conv1)\n",
    "\n",
    "flatten = Flatten(data_format='channels_last',name='Flatten')(Conv2)\n",
    "\n",
    "#output layer\n",
    "Out = Dense(units=16,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=3),name='Output')(flatten)\n",
    "\n",
    "#Creating a model\n",
    "model3 = Model(inputs=input_layer, outputs=Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model3, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# !rmdir /s /q logs\\model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "log_dir=\"logs\\\\model3\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "model3.fit_generator(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=216,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.evaluate_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Please write your observations at the end of every model \n",
    "\n",
    "# 2.Please write your analysis of tensorboard results for each model.\n",
    "\n",
    "# 4.For model 1 and model 2, you will get  >60 % validation accuracy, please recheck your code."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
