{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introductory-transaction",
   "metadata": {
    "colab_type": "text",
    "id": "AQDRNrY2NCXf"
   },
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "likely-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing liberaries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contemporary-disposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (20000, 2) (20000,)\n",
      "Train data shape:  (14000, 2) (14000,)\n",
      "CV data shape:     (6000, 2) (6000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2  label\n",
       "0  0.450564  1.074305      0\n",
       "1  0.085632  0.967682      0\n",
       "2  0.117326  0.971521      1\n",
       "3  0.982179 -0.380408      0\n",
       "4 -0.720352  0.955850      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data\n",
    "data = pd.read_csv('data.csv')\n",
    "data.label = data.label.apply(lambda x: int(x))\n",
    "x = data[['f1', 'f2']]\n",
    "y = data.label\n",
    "print(\"Data shape: \", x.shape, y.shape)\n",
    "data.head()\n",
    "\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Train data shape: \", x_train.shape, y_train.shape)\n",
    "print(\"CV data shape:    \", x_cv.shape, y_cv.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "postal-arrangement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "1    10000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking wether the data is balanced or not\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indie-slave",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr, pre_vcc, cur_val_acc):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        if cur_val_acc < pre_vcc:\n",
    "            lr = 0.9 * lr\n",
    "        if (epoch+1) % 3 == 0:\n",
    "            lr = 0.95 * lr\n",
    "    return lr\n",
    "\n",
    "# callback to find metrics on epoch end\n",
    "class Metrics(Callback):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.history = {'epoch':[], 'learning_rate':[], 'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'auc': [], 'f1_micro': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_hat_pred = np.asarray(self.model.predict(self.x))\n",
    "        y_hat = np.where(y_hat_pred > 0.5, 1, 0)\n",
    "        \n",
    "        self.history['epoch'].append(epoch+1)\n",
    "        \n",
    "        # Terminating the training if loss is NaN\n",
    "        if np.isnan(logs.get('loss', np.nan)):\n",
    "            print('model stoped training, because loss found to be NaN...')\n",
    "            sef.model.stop_training = True\n",
    "        else:\n",
    "            self.history['loss'].append(round(logs.get('loss'), 4))\n",
    "        \n",
    "        self.history['acc'].append(round(logs.get('acc'), 4))\n",
    "        \n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(round(logs.get('val_loss'), 4))\n",
    "        \n",
    "        \n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(round(logs.get('val_acc'), 4))\n",
    "         \n",
    "        # finding auc and micro f1_score\n",
    "        auc = round(roc_auc_score(self.y, y_hat_pred), 4)\n",
    "        f1_micro = round(f1_score(self.y, y_hat, average='micro'), 4)\n",
    "        self.history['auc'].append(auc)\n",
    "        self.history['f1_micro'].append(f1_micro)\n",
    "        print('\\nauc: {}    f1_micro: {}'.format(auc, f1_micro))\n",
    "        \n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        \n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        \n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        if epoch != 0:\n",
    "            scheduled_lr = lr_schedule(epoch, lr, self.history['acc'][-2], self.history['val_acc'][-1])\n",
    "        else:\n",
    "            scheduled_lr = lr\n",
    "        \n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nLearning rate is %6.4f.\" % (scheduled_lr))\n",
    "        \n",
    "        # Terminating the training if any of the weight are NaN\n",
    "        for weights in self.model.get_weights():\n",
    "            if np.isnan(np.sum(weights)):\n",
    "                print(\"model stoped training, because any of the weight found to be NaN...\")\n",
    "                sef.model.stop_training = True\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-meter",
   "metadata": {},
   "source": [
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "martial-breeding",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "19552/20000 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5215\n",
      "auc: 0.5058    f1_micro: 0.5242\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52417, saving model to model_save/model1/weights-01-0.5242.hdf5\n",
      "20000/20000 [==============================] - 4s 181us/sample - loss: 0.6930 - acc: 0.5209 - val_loss: 0.6881 - val_acc: 0.5242\n",
      "Epoch 2/50\n",
      "19488/20000 [============================>.] - ETA: 0s - loss: 0.6877 - acc: 0.5223\n",
      "auc: 0.5057    f1_micro: 0.5318\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.52417 to 0.53183, saving model to model_save/model1/weights-02-0.5318.hdf5\n",
      "20000/20000 [==============================] - 1s 71us/sample - loss: 0.6877 - acc: 0.5228 - val_loss: 0.6842 - val_acc: 0.5318\n",
      "Epoch 3/50\n",
      "19200/20000 [===========================>..] - ETA: 0s - loss: 0.6874 - acc: 0.5232\n",
      "auc: 0.5071    f1_micro: 0.5405\n",
      "\n",
      "Learning rate is 0.0950.\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53183 to 0.54050, saving model to model_save/model1/weights-03-0.5405.hdf5\n",
      "20000/20000 [==============================] - 1s 73us/sample - loss: 0.6870 - acc: 0.5260 - val_loss: 0.6910 - val_acc: 0.5405\n",
      "Epoch 4/50\n",
      "19936/20000 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.5349\n",
      "auc: 0.5203    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0855.\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.54050\n",
      "20000/20000 [==============================] - 2s 76us/sample - loss: 0.6855 - acc: 0.5349 - val_loss: 0.6936 - val_acc: 0.5032\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.6881</td>\n",
       "      <td>0.5242</td>\n",
       "      <td>0.5058</td>\n",
       "      <td>0.5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6877</td>\n",
       "      <td>0.5228</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>0.5318</td>\n",
       "      <td>0.5057</td>\n",
       "      <td>0.5318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6870</td>\n",
       "      <td>0.5260</td>\n",
       "      <td>0.6910</td>\n",
       "      <td>0.5405</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.5405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.6855</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.6936</td>\n",
       "      <td>0.5032</td>\n",
       "      <td>0.5203</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  learning_rate    loss     acc  val_loss  val_acc     auc  f1_micro\n",
       "0      1          0.100  0.6930  0.5209    0.6881   0.5242  0.5058    0.5242\n",
       "1      2          0.100  0.6877  0.5228    0.6842   0.5318  0.5057    0.5318\n",
       "2      3          0.100  0.6870  0.5260    0.6910   0.5405  0.5071    0.5405\n",
       "3      4          0.095  0.6855  0.5349    0.6936   0.5032  0.5203    0.5032"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(4, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(2, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.RandomUniform(0,1))\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "filepath=\"model_save/model1/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "\n",
    "\n",
    "log_dir=\"logs\\\\model1\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop]#, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x, y, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "regular-football",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8592), started 0:41:08 ago. (Use '!kill 8592' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dd70619cf678aec3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dd70619cf678aec3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/model\n",
    "# # %tensorboard --logdir logs/model1 --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-grenada",
   "metadata": {},
   "source": [
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Anamyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lovely-legislation",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "19264/20000 [===========================>..] - ETA: 0s - loss: 0.7002 - acc: 0.5009"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'flatten_input' with dtype float and shape [?,2]\n\t [[{{node flatten_input}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8b3f3c879241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmetrics_binary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlystop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mbinary_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m           steps_name='validation_steps')\n\u001b[0m\u001b[0;32m    441\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'flatten_input' with dtype float and shape [?,2]\n\t [[{{node flatten_input}}]]"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.RandomUniform(0,1))\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "filepath=\"model_save/model2/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "\n",
    "\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "\n",
    "\n",
    "log_dir=\"logs\\\\model2\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x, y, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/sgd_moment\n",
    "# # %tensorboard --logdir logs/sgd_moment --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-hollywood",
   "metadata": {},
   "source": [
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "private-naples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6950 - acc: 0.4980\n",
      "auc: 0.5    f1_micro: 0.4968\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49683, saving model to model_save/weights-01-0.4968.hdf5\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.6949 - acc: 0.4983 - val_loss: 0.6946 - val_acc: 0.4968\n",
      "Epoch 2/50\n",
      "19232/20000 [===========================>..] - ETA: 0s - loss: 0.6954 - acc: 0.5030\n",
      "auc: 0.5    f1_micro: 0.4968\n",
      "\n",
      "Learning rate is 0.0900.\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.49683\n",
      "20000/20000 [==============================] - 1s 71us/sample - loss: 0.6955 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.4968\n",
      "Epoch 3/50\n",
      "19296/20000 [===========================>..] - ETA: 0s - loss: 0.6941 - acc: 0.5047\n",
      "auc: 0.5    f1_micro: 0.4968\n",
      "\n",
      "Learning rate is 0.0770.\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.49683\n",
      "20000/20000 [==============================] - 1s 73us/sample - loss: 0.6943 - acc: 0.5044 - val_loss: 0.6961 - val_acc: 0.4968\n",
      "Epoch 4/50\n",
      "19424/20000 [============================>.] - ETA: 0s - loss: 0.6945 - acc: 0.5034\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0693.\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.49683 to 0.50317, saving model to model_save/weights-04-0.5032.hdf5\n",
      "20000/20000 [==============================] - 2s 79us/sample - loss: 0.6945 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 5/50\n",
      "19520/20000 [============================>.] - ETA: 0s - loss: 0.6946 - acc: 0.4967\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0693.\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50317\n",
      "20000/20000 [==============================] - 2s 88us/sample - loss: 0.6946 - acc: 0.4971 - val_loss: 0.6953 - val_acc: 0.5032\n",
      "Epoch 6/50\n",
      "19552/20000 [============================>.] - ETA: 0s - loss: 0.6943 - acc: 0.4963\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0658.\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50317\n",
      "20000/20000 [==============================] - 2s 79us/sample - loss: 0.6943 - acc: 0.4963 - val_loss: 0.6934 - val_acc: 0.5032\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.6949</td>\n",
       "      <td>0.4983</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.5023</td>\n",
       "      <td>0.6932</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.6943</td>\n",
       "      <td>0.5044</td>\n",
       "      <td>0.6961</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076950</td>\n",
       "      <td>0.6945</td>\n",
       "      <td>0.5023</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.5032</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.069255</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.4971</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.5032</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.069255</td>\n",
       "      <td>0.6943</td>\n",
       "      <td>0.4963</td>\n",
       "      <td>0.6934</td>\n",
       "      <td>0.5032</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate    loss     acc  val_loss  val_acc  auc  f1_micro\n",
       "1       0.100000  0.6949  0.4983    0.6946   0.4968  0.5    0.4968\n",
       "2       0.100000  0.6955  0.5023    0.6932   0.4968  0.5    0.4968\n",
       "3       0.090000  0.6943  0.5044    0.6961   0.4968  0.5    0.4968\n",
       "4       0.076950  0.6945  0.5023    0.6931   0.5032  0.5    0.5032\n",
       "5       0.069255  0.6946  0.4971    0.6953   0.5032  0.5    0.5032\n",
       "6       0.069255  0.6943  0.4963    0.6934   0.5032  0.5    0.5032"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.he_uniform())\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "filepath=\"model_save/model3/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "\n",
    "\n",
    "log_dir=\"logs\\\\model3\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x, y, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-builder",
   "metadata": {},
   "source": [
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-identification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-welcome",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-disposal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-hardware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-jenny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-calendar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
