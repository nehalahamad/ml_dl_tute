{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQDRNrY2NCXf"
   },
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ab6ff71f928d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# importing liberaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# import tensorflow as tf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import numpy as np\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# importing liberaries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (20000, 2) (20000,)\n"
     ]
    }
   ],
   "source": [
    "# importing data\n",
    "data = pd.read_csv('data.csv')\n",
    "data.label = data.label.apply(lambda x: int(x))\n",
    "x = data[['f1', 'f2']]\n",
    "y = data.label\n",
    "print(\"Data shape: \", x.shape, y.shape)\n",
    "data.head()\n",
    "\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (14000, 2) (14000,)\n",
      "CV data shape:     (6000, 2) (6000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data shape: \", x_train.shape, y_train.shape)\n",
    "print(\"CV data shape:    \", x_cv.shape, y_cv.shape)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10000\n",
      "1    10000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking wether the data is balanced or not\n",
    "print(data.label.value_counts())\n",
    "\n",
    "def lr_schedule(epoch, lr, pre_vcc, cur_val_acc):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if cur_val_acc < pre_vcc:\n",
    "        lr = 0.9 * lr\n",
    "    if (epoch+1) % 3 == 0:\n",
    "        lr = 0.95 * lr\n",
    "    return lr\n",
    "\n",
    "# callback to find metrics on epoch end\n",
    "class Metrics(Callback):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.history={'epoch':[], 'learning_rate':[], 'loss':[],'acc':[], 'val_loss':[], 'val_acc':[], 'auc':[], 'f1_micro':[]}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_hat_pred = np.asarray(self.model.predict(self.x))\n",
    "        y_hat = np.where(y_hat_pred > 0.5, 1, 0)\n",
    "        \n",
    "        self.history['epoch'].append(epoch+1)\n",
    "        \n",
    "        # Terminating the training if loss is NaN\n",
    "        if np.isnan(logs.get('loss', np.nan)):\n",
    "            print('model stoped training, because loss found to be NaN...')\n",
    "            sef.model.stop_training = True\n",
    "        else:\n",
    "            self.history['loss'].append(logs.get('loss'))\n",
    "        \n",
    "        self.history['acc'].append(logs.get('acc'))\n",
    "        \n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        \n",
    "        \n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(logs.get('val_acc'))\n",
    "         \n",
    "        # finding auc and micro f1_score\n",
    "        auc = round(roc_auc_score(self.y, y_hat_pred), 4)\n",
    "        f1_micro = round(f1_score(self.y, y_hat, average='micro'), 4)\n",
    "        self.history['auc'].append(auc)\n",
    "        self.history['f1_micro'].append(f1_micro)\n",
    "        print('\\nauc: {}    f1_micro: {}'.format(auc, f1_micro))\n",
    "        \n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        \n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        \n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        if epoch != 0:\n",
    "            scheduled_lr = lr_schedule(epoch, lr, self.history['acc'][-2], self.history['val_acc'][-1])\n",
    "        else:\n",
    "            scheduled_lr = lr\n",
    "        \n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nLearning rate is %6.4f.\" % (scheduled_lr))\n",
    "        \n",
    "        # Terminating the training if any of the weight are NaN\n",
    "        for weights in self.model.get_weights():\n",
    "            if np.isnan(np.sum(weights)):\n",
    "                print(\"model stoped training, because any of the weight found to be NaN...\")\n",
    "                sef.model.stop_training = True\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "!rmdir /s /q logs\\model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "   32/14000 [..............................] - ETA: 3:34 - loss: 0.7381 - acc: 0.6250WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.104028). Check your callbacks.\n",
      "13888/14000 [============================>.] - ETA: 0s - loss: 0.6983 - acc: 0.5036\n",
      "auc: 0.4996    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50317, saving model to model_save/model1/weights-01-0.5032.hdf5\n",
      "14000/14000 [==============================] - 9s 669us/sample - loss: 0.6984 - acc: 0.5033 - val_loss: 0.7000 - val_acc: 0.5032\n",
      "Epoch 2/50\n",
      "13856/14000 [============================>.] - ETA: 0s - loss: 0.6960 - acc: 0.5065\n",
      "auc: 0.5284    f1_micro: 0.4968\n",
      "\n",
      "Learning rate is 0.0900.\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50317\n",
      "14000/14000 [==============================] - 8s 572us/sample - loss: 0.6960 - acc: 0.5069 - val_loss: 0.6962 - val_acc: 0.4968\n",
      "Epoch 3/50\n",
      "13952/14000 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.5173\n",
      "auc: 0.5446    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0770.\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50317\n",
      "14000/14000 [==============================] - 8s 544us/sample - loss: 0.6934 - acc: 0.5168 - val_loss: 0.6874 - val_acc: 0.5032\n",
      "Epoch 4/50\n",
      "13856/14000 [============================>.] - ETA: 0s - loss: 0.6901 - acc: 0.5227\n",
      "auc: 0.5377    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0693.\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50317\n",
      "14000/14000 [==============================] - 7s 484us/sample - loss: 0.6900 - acc: 0.5222 - val_loss: 0.6892 - val_acc: 0.5032\n",
      "Epoch 5/50\n",
      "13856/14000 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.5224\n",
      "auc: 0.5398    f1_micro: 0.5378\n",
      "\n",
      "Learning rate is 0.0693.\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.50317 to 0.53783, saving model to model_save/model1/weights-05-0.5378.hdf5\n",
      "14000/14000 [==============================] - 7s 512us/sample - loss: 0.6907 - acc: 0.5227 - val_loss: 0.6884 - val_acc: 0.5378\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.698392</td>\n",
       "      <td>0.503286</td>\n",
       "      <td>0.699961</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.4996</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.695977</td>\n",
       "      <td>0.506857</td>\n",
       "      <td>0.696202</td>\n",
       "      <td>0.496833</td>\n",
       "      <td>0.5284</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.693431</td>\n",
       "      <td>0.516786</td>\n",
       "      <td>0.687407</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5446</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.076950</td>\n",
       "      <td>0.690001</td>\n",
       "      <td>0.522214</td>\n",
       "      <td>0.689233</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5377</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.069255</td>\n",
       "      <td>0.690691</td>\n",
       "      <td>0.522714</td>\n",
       "      <td>0.688394</td>\n",
       "      <td>0.537833</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  learning_rate      loss       acc  val_loss   val_acc     auc  \\\n",
       "0      1       0.100000  0.698392  0.503286  0.699961  0.503167  0.4996   \n",
       "1      2       0.100000  0.695977  0.506857  0.696202  0.496833  0.5284   \n",
       "2      3       0.090000  0.693431  0.516786  0.687407  0.503167  0.5446   \n",
       "3      4       0.076950  0.690001  0.522214  0.689233  0.503167  0.5377   \n",
       "4      5       0.069255  0.690691  0.522714  0.688394  0.537833  0.5398   \n",
       "\n",
       "   f1_micro  \n",
       "0    0.5032  \n",
       "1    0.4968  \n",
       "2    0.5032  \n",
       "3    0.5032  \n",
       "4    0.5378  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(16, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(16, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(8, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(2, activation='tanh', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.RandomUniform(0,1))\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "# 1\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "# 2\n",
    "filepath=\"model_save/model1/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "# 3\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "# 4\n",
    "log_dir=\"logs\\\\model1\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x_train, y_train, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Anamyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "!rmdir /s /q logs\\model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 20000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "19712/20000 [============================>.] - ETA: 0s - loss: 0.8349 - acc: 0.4995\n",
      "auc: 0.5    f1_micro: 0.4968\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49683, saving model to model_save/model2/weights-01-0.4968.hdf5\n",
      "20000/20000 [==============================] - 9s 459us/sample - loss: 0.8327 - acc: 0.5005 - val_loss: 0.6995 - val_acc: 0.4968\n",
      "Epoch 2/50\n",
      "19680/20000 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.5046\n",
      "auc: 0.5    f1_micro: 0.4968\n",
      "\n",
      "Learning rate is 0.0900.\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.49683\n",
      "20000/20000 [==============================] - 7s 356us/sample - loss: 0.6949 - acc: 0.5049 - val_loss: 0.7039 - val_acc: 0.4968\n",
      "Epoch 3/50\n",
      "19808/20000 [============================>.] - ETA: 0s - loss: 0.6950 - acc: 0.5008\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0770.\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.49683 to 0.50317, saving model to model_save/model2/weights-03-0.5032.hdf5\n",
      "20000/20000 [==============================] - 7s 374us/sample - loss: 0.6950 - acc: 0.5007 - val_loss: 0.6944 - val_acc: 0.5032\n",
      "Epoch 4/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6948 - acc: 0.4985\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0770.\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50317\n",
      "20000/20000 [==============================] - 7s 348us/sample - loss: 0.6948 - acc: 0.4984 - val_loss: 0.6939 - val_acc: 0.5032\n",
      "Epoch 5/50\n",
      "19712/20000 [============================>.] - ETA: 0s - loss: 0.6953 - acc: 0.4969\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0770.\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50317\n",
      "20000/20000 [==============================] - 7s 362us/sample - loss: 0.6954 - acc: 0.4958 - val_loss: 0.6935 - val_acc: 0.5032\n",
      "Epoch 6/50\n",
      "19840/20000 [============================>.] - ETA: 0s - loss: 0.6943 - acc: 0.4994\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0731.\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50317\n",
      "20000/20000 [==============================] - 7s 356us/sample - loss: 0.6944 - acc: 0.4991 - val_loss: 0.6950 - val_acc: 0.5032\n",
      "Epoch 7/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6951 - acc: 0.4984\n",
      "auc: 0.5    f1_micro: 0.5032\n",
      "\n",
      "Learning rate is 0.0731.\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50317\n",
      "20000/20000 [==============================] - 8s 381us/sample - loss: 0.6951 - acc: 0.4984 - val_loss: 0.6974 - val_acc: 0.5032\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.832745</td>\n",
       "      <td>0.50055</td>\n",
       "      <td>0.699503</td>\n",
       "      <td>0.496833</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.694909</td>\n",
       "      <td>0.50490</td>\n",
       "      <td>0.703906</td>\n",
       "      <td>0.496833</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.695010</td>\n",
       "      <td>0.50070</td>\n",
       "      <td>0.694375</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.076950</td>\n",
       "      <td>0.694826</td>\n",
       "      <td>0.49840</td>\n",
       "      <td>0.693858</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.076950</td>\n",
       "      <td>0.695361</td>\n",
       "      <td>0.49580</td>\n",
       "      <td>0.693469</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.076950</td>\n",
       "      <td>0.694364</td>\n",
       "      <td>0.49910</td>\n",
       "      <td>0.695017</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.073103</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>0.49840</td>\n",
       "      <td>0.697387</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  learning_rate      loss      acc  val_loss   val_acc  auc  f1_micro\n",
       "0      1       0.100000  0.832745  0.50055  0.699503  0.496833  0.5    0.4968\n",
       "1      2       0.100000  0.694909  0.50490  0.703906  0.496833  0.5    0.4968\n",
       "2      3       0.090000  0.695010  0.50070  0.694375  0.503167  0.5    0.5032\n",
       "3      4       0.076950  0.694826  0.49840  0.693858  0.503167  0.5    0.5032\n",
       "4      5       0.076950  0.695361  0.49580  0.693469  0.503167  0.5    0.5032\n",
       "5      6       0.076950  0.694364  0.49910  0.695017  0.503167  0.5    0.5032\n",
       "6      7       0.073103  0.695100  0.49840  0.697387  0.503167  0.5    0.5032"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(16, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(8, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(0,1)),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.RandomUniform(0,1))\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "filepath=\"model_save/model2/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "\n",
    "\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "\n",
    "\n",
    "log_dir=\"logs\\\\model2\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x, y, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "!rmdir /s /q logs\\model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Nehal\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 20000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "19712/20000 [============================>.] - ETA: 0s - loss: 0.6524 - acc: 0.6187\n",
      "auc: 0.6987    f1_micro: 0.6333\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63333, saving model to model_save/model3/weights-01-0.6333.hdf5\n",
      "20000/20000 [==============================] - 10s 512us/sample - loss: 0.6519 - acc: 0.6191 - val_loss: 0.6386 - val_acc: 0.6333\n",
      "Epoch 2/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6338 - acc: 0.6485\n",
      "auc: 0.7164    f1_micro: 0.6585\n",
      "\n",
      "Learning rate is 0.1000.\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63333 to 0.65850, saving model to model_save/model3/weights-02-0.6585.hdf5\n",
      "20000/20000 [==============================] - 10s 511us/sample - loss: 0.6342 - acc: 0.6481 - val_loss: 0.6309 - val_acc: 0.6585\n",
      "Epoch 3/50\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.6410 - acc: 0.6367\n",
      "auc: 0.7052    f1_micro: 0.6387\n",
      "\n",
      "Learning rate is 0.0855.\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65850\n",
      "20000/20000 [==============================] - 10s 498us/sample - loss: 0.6410 - acc: 0.6367 - val_loss: 0.6321 - val_acc: 0.6387\n",
      "Epoch 4/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6291 - acc: 0.6541\n",
      "auc: 0.7156    f1_micro: 0.6313\n",
      "\n",
      "Learning rate is 0.0770.\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.65850\n",
      "20000/20000 [==============================] - 10s 496us/sample - loss: 0.6288 - acc: 0.6542 - val_loss: 0.6432 - val_acc: 0.6313\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.651884</td>\n",
       "      <td>0.61915</td>\n",
       "      <td>0.638589</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.6987</td>\n",
       "      <td>0.6333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.634159</td>\n",
       "      <td>0.64810</td>\n",
       "      <td>0.630876</td>\n",
       "      <td>0.658500</td>\n",
       "      <td>0.7164</td>\n",
       "      <td>0.6585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.641033</td>\n",
       "      <td>0.63675</td>\n",
       "      <td>0.632052</td>\n",
       "      <td>0.638667</td>\n",
       "      <td>0.7052</td>\n",
       "      <td>0.6387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.628830</td>\n",
       "      <td>0.65415</td>\n",
       "      <td>0.643232</td>\n",
       "      <td>0.631333</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.6313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  learning_rate      loss      acc  val_loss   val_acc     auc  \\\n",
       "0      1         0.1000  0.651884  0.61915  0.638589  0.633333  0.6987   \n",
       "1      2         0.1000  0.634159  0.64810  0.630876  0.658500  0.7164   \n",
       "2      3         0.1000  0.641033  0.63675  0.632052  0.638667  0.7052   \n",
       "3      4         0.0855  0.628830  0.65415  0.643232  0.631333  0.7156   \n",
       "\n",
       "   f1_micro  \n",
       "0    0.6333  \n",
       "1    0.6585  \n",
       "2    0.6387  \n",
       "3    0.6313  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(16, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(8, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(4, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform()),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.he_uniform())\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "filepath=\"model_save/model3/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "\n",
    "\n",
    "log_dir=\"logs\\\\model3\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x, y, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "!rmdir /s /q logs\\model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "19936/20000 [============================>.] - ETA: 0s - loss: 0.8505 - acc: 0.6144\n",
      "auc: 0.7119    f1_micro: 0.6535\n",
      "\n",
      "Learning rate is 0.0010.\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65350, saving model to model_save/model4/weights-01-0.6535.hdf5\n",
      "20000/20000 [==============================] - 12s 602us/sample - loss: 0.8499 - acc: 0.6147 - val_loss: 0.6511 - val_acc: 0.6535\n",
      "Epoch 2/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6412 - acc: 0.6626\n",
      "auc: 0.716    f1_micro: 0.6523\n",
      "\n",
      "Learning rate is 0.0010.\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.65350\n",
      "20000/20000 [==============================] - 9s 452us/sample - loss: 0.6413 - acc: 0.6622 - val_loss: 0.6374 - val_acc: 0.6523\n",
      "Epoch 3/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6250 - acc: 0.6651\n",
      "auc: 0.7132    f1_micro: 0.6527\n",
      "\n",
      "Learning rate is 0.0009.\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65350\n",
      "20000/20000 [==============================] - 9s 462us/sample - loss: 0.6250 - acc: 0.6651 - val_loss: 0.6283 - val_acc: 0.6527\n",
      "Epoch 4/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6140 - acc: 0.6658\n",
      "auc: 0.7174    f1_micro: 0.6645\n",
      "\n",
      "Learning rate is 0.0008.\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65350 to 0.66450, saving model to model_save/model4/weights-04-0.6645.hdf5\n",
      "20000/20000 [==============================] - 10s 505us/sample - loss: 0.6140 - acc: 0.6658 - val_loss: 0.6188 - val_acc: 0.6645\n",
      "Epoch 5/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6100 - acc: 0.6659\n",
      "auc: 0.717    f1_micro: 0.6627\n",
      "\n",
      "Learning rate is 0.0007.\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.66450\n",
      "20000/20000 [==============================] - 10s 481us/sample - loss: 0.6099 - acc: 0.6660 - val_loss: 0.6178 - val_acc: 0.6627\n",
      "Epoch 6/50\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.6079 - acc: 0.6692\n",
      "auc: 0.7243    f1_micro: 0.6633\n",
      "\n",
      "Learning rate is 0.0006.\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.66450\n",
      "20000/20000 [==============================] - 9s 472us/sample - loss: 0.6080 - acc: 0.6690 - val_loss: 0.6151 - val_acc: 0.6633\n",
      "Epoch 7/50\n",
      "19744/20000 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.6711\n",
      "auc: 0.7237    f1_micro: 0.6642\n",
      "\n",
      "Learning rate is 0.0005.\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.66450\n",
      "20000/20000 [==============================] - 10s 505us/sample - loss: 0.6046 - acc: 0.6719 - val_loss: 0.6122 - val_acc: 0.6642\n",
      "Epoch 8/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6040 - acc: 0.6739\n",
      "auc: 0.724    f1_micro: 0.6652\n",
      "\n",
      "Learning rate is 0.0005.\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.66450 to 0.66517, saving model to model_save/model4/weights-08-0.6652.hdf5\n",
      "20000/20000 [==============================] - 9s 430us/sample - loss: 0.6039 - acc: 0.6738 - val_loss: 0.6121 - val_acc: 0.6652\n",
      "Epoch 9/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.6690\n",
      "auc: 0.7247    f1_micro: 0.6638\n",
      "\n",
      "Learning rate is 0.0004.\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.66517\n",
      "20000/20000 [==============================] - 9s 467us/sample - loss: 0.6035 - acc: 0.6690 - val_loss: 0.6114 - val_acc: 0.6638\n",
      "Epoch 10/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.6699\n",
      "auc: 0.7235    f1_micro: 0.6633\n",
      "\n",
      "Learning rate is 0.0004.\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.66517\n",
      "20000/20000 [==============================] - 10s 492us/sample - loss: 0.6029 - acc: 0.6700 - val_loss: 0.6123 - val_acc: 0.6633\n",
      "Epoch 11/50\n",
      "19872/20000 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.6686\n",
      "auc: 0.725    f1_micro: 0.6667\n",
      "\n",
      "Learning rate is 0.0003.\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.66517 to 0.66667, saving model to model_save/model4/weights-11-0.6667.hdf5\n",
      "20000/20000 [==============================] - 9s 455us/sample - loss: 0.6021 - acc: 0.6683 - val_loss: 0.6112 - val_acc: 0.6667\n",
      "Epoch 12/50\n",
      "19776/20000 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.6708\n",
      "auc: 0.7235    f1_micro: 0.6672\n",
      "\n",
      "Learning rate is 0.0003.\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.66667 to 0.66717, saving model to model_save/model4/weights-12-0.6672.hdf5\n",
      "20000/20000 [==============================] - 9s 439us/sample - loss: 0.6021 - acc: 0.6705 - val_loss: 0.6127 - val_acc: 0.6672\n",
      "Epoch 13/50\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.6705- ETA: 0s - loss: 0.6012 - acc: 0.671 - ETA: 0s - loss: 0.601\n",
      "auc: 0.7251    f1_micro: 0.6673\n",
      "\n",
      "Learning rate is 0.0003.\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.66717 to 0.66733, saving model to model_save/model4/weights-13-0.6673.hdf5\n",
      "20000/20000 [==============================] - 12s 606us/sample - loss: 0.6018 - acc: 0.6704 - val_loss: 0.6111 - val_acc: 0.6673\n",
      "Epoch 14/50\n",
      "19744/20000 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.6710\n",
      "auc: 0.7251    f1_micro: 0.6653\n",
      "\n",
      "Learning rate is 0.0002.\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.66733\n",
      "20000/20000 [==============================] - 10s 498us/sample - loss: 0.6011 - acc: 0.6711 - val_loss: 0.6111 - val_acc: 0.6653\n",
      "Epoch 15/50\n",
      "19840/20000 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.6703\n",
      "auc: 0.7233    f1_micro: 0.6623\n",
      "\n",
      "Learning rate is 0.0002.\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.66733\n",
      "20000/20000 [==============================] - 10s 498us/sample - loss: 0.6009 - acc: 0.6704 - val_loss: 0.6140 - val_acc: 0.6623\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.849896</td>\n",
       "      <td>0.61465</td>\n",
       "      <td>0.651069</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.6535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.641288</td>\n",
       "      <td>0.66225</td>\n",
       "      <td>0.637434</td>\n",
       "      <td>0.652333</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.6523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.624956</td>\n",
       "      <td>0.66505</td>\n",
       "      <td>0.628262</td>\n",
       "      <td>0.652667</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.6527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.614023</td>\n",
       "      <td>0.66575</td>\n",
       "      <td>0.618836</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>0.7174</td>\n",
       "      <td>0.6645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.609872</td>\n",
       "      <td>0.66600</td>\n",
       "      <td>0.617813</td>\n",
       "      <td>0.662667</td>\n",
       "      <td>0.7170</td>\n",
       "      <td>0.6627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.607980</td>\n",
       "      <td>0.66905</td>\n",
       "      <td>0.615071</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.7243</td>\n",
       "      <td>0.6633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.604588</td>\n",
       "      <td>0.67185</td>\n",
       "      <td>0.612153</td>\n",
       "      <td>0.664167</td>\n",
       "      <td>0.7237</td>\n",
       "      <td>0.6642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.67380</td>\n",
       "      <td>0.612106</td>\n",
       "      <td>0.665167</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.603468</td>\n",
       "      <td>0.66900</td>\n",
       "      <td>0.611423</td>\n",
       "      <td>0.663833</td>\n",
       "      <td>0.7247</td>\n",
       "      <td>0.6638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.602898</td>\n",
       "      <td>0.66995</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>0.6633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.602098</td>\n",
       "      <td>0.66835</td>\n",
       "      <td>0.611244</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.602064</td>\n",
       "      <td>0.67055</td>\n",
       "      <td>0.612742</td>\n",
       "      <td>0.667167</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>0.6672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.601815</td>\n",
       "      <td>0.67045</td>\n",
       "      <td>0.611058</td>\n",
       "      <td>0.667333</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>0.6673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.601141</td>\n",
       "      <td>0.67110</td>\n",
       "      <td>0.611067</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>0.6653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.600937</td>\n",
       "      <td>0.67040</td>\n",
       "      <td>0.613959</td>\n",
       "      <td>0.662333</td>\n",
       "      <td>0.7233</td>\n",
       "      <td>0.6623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  learning_rate      loss      acc  val_loss   val_acc     auc  \\\n",
       "0       1       0.001000  0.849896  0.61465  0.651069  0.653500  0.7119   \n",
       "1       2       0.001000  0.641288  0.66225  0.637434  0.652333  0.7160   \n",
       "2       3       0.001000  0.624956  0.66505  0.628262  0.652667  0.7132   \n",
       "3       4       0.000855  0.614023  0.66575  0.618836  0.664500  0.7174   \n",
       "4       5       0.000770  0.609872  0.66600  0.617813  0.662667  0.7170   \n",
       "5       6       0.000693  0.607980  0.66905  0.615071  0.663333  0.7243   \n",
       "6       7       0.000592  0.604588  0.67185  0.612153  0.664167  0.7237   \n",
       "7       8       0.000533  0.603922  0.67380  0.612106  0.665167  0.7240   \n",
       "8       9       0.000480  0.603468  0.66900  0.611423  0.663833  0.7247   \n",
       "9      10       0.000410  0.602898  0.66995  0.612277  0.663333  0.7235   \n",
       "10     11       0.000369  0.602098  0.66835  0.611244  0.666667  0.7250   \n",
       "11     12       0.000332  0.602064  0.67055  0.612742  0.667167  0.7235   \n",
       "12     13       0.000284  0.601815  0.67045  0.611058  0.667333  0.7251   \n",
       "13     14       0.000256  0.601141  0.67110  0.611067  0.665333  0.7251   \n",
       "14     15       0.000230  0.600937  0.67040  0.613959  0.662333  0.7233   \n",
       "\n",
       "    f1_micro  \n",
       "0     0.6535  \n",
       "1     0.6523  \n",
       "2     0.6527  \n",
       "3     0.6645  \n",
       "4     0.6627  \n",
       "5     0.6633  \n",
       "6     0.6642  \n",
       "7     0.6652  \n",
       "8     0.6638  \n",
       "9     0.6633  \n",
       "10    0.6667  \n",
       "11    0.6672  \n",
       "12    0.6673  \n",
       "13    0.6653  \n",
       "14    0.6623  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        # input layer\n",
    "        tf.keras.layers.Flatten(input_shape=(2,)),\n",
    "        # Hidden layars\n",
    "        tf.keras.layers.Dense(32, activation='tanh', kernel_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(16, activation='tanh', kernel_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(8, activation='tanh', kernel_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(4, activation='tanh', kernel_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(2, activation='tanh', kernel_initializer='glorot_uniform'),\n",
    "        # output layer\n",
    "        tf.keras.layers.Dense(1, activation='tanh', kernel_initializer='glorot_uniform')\n",
    "  ])\n",
    "\n",
    "binary_model = create_model()\n",
    "binary_model.compile(optimizer = 'adam',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "filepath=\"model_save/model4/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1)\n",
    "\n",
    "metrics_binary = Metrics(x_cv, y_cv)\n",
    "\n",
    "\n",
    "log_dir=\"logs\\\\model4\\\\\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "\n",
    "callbacks_list = [metrics_binary, checkpoint, earlystop, tensorboard_callback]\n",
    "\n",
    "binary_model.fit(x, y, epochs=50, validation_data=(x_cv, y_cv), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "pd.DataFrame(metrics_binary.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
